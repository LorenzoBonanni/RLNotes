\section{Policy Gradient Methods}
So far, all the methods we have discussed are action-value methods. These methods estimate action values and use them to select actions, but they do not explicitly represent the policy function.\\

In contrast, {policy gradient methods take a different approach. They learn a parameterized policy function $\pi(a|s, \theta)$and use this function to select actions without relying on value functions. Although a value function can still be used to learn the policy parameter $\bm{\theta}$, it is not necessary for action selection.

Actor Critic methods are a type of policy gradient methods that also approximate the value function. They consist of two main components:
\begin{itemize}
    \item Actor: learn policy function
    \item Critic: learn value function
\end{itemize}

Before we proceed, let's introduce some notation:
\begin{itemize}
    \item The policy's parameter vector is denoted as $\bm{\theta}\in\mathbb{R}^{d'}$.
    \item $\pi(a|s, \bm{\theta})$ represents the probability of selecting action $a$ given state $s$ and policy parameters $\bm{\theta}$. In other words, $\pi(a|s, \bm{\theta})=Pr\{A_t=a|S_t=s,\bm{\theta}_t=\bm{\theta}\}$.
    \item If the method requires it, the learned value function is denoted as $\hat{v}(s, \bm{w})$ and it has parameters $\bm{w}\in\mathbb{R}^{d}$.
    \item $J(\bm{\theta})$ denotes a measure of policy performance, which depends on the policy parameters."
\end{itemize}

The objective of policy gradient methods is to find the optimal parameters $\bm{\theta}$ that maximize the performance measure $J(\bm{\theta})$.

The parameter updates in these methods can be approximated using gradient ascent, as follows:

\begin{align}
\bm{\theta}_{t+1} = \bm{\theta}_t + \alpha \widehat{\nabla J(\bm{\theta}_t})
\end{align}

Here, $\widehat{\nabla J(\bm{\theta}_t})\in \mathbb{R}^{d'}$ represents a stochastic estimate of the gradient of $J(\bm{\theta})$ with respect to $\bm{\theta}_t$. The learning rate is denoted by $\alpha$.\\

In the episodic case, the performance is measured by the value of the starting state under the parameterized policy. In contrast, in the continuing case, performance is assessed based on the average reward rate.

\subsection{Policy Approximation and its Advantages}
In policy gradient methods, the policy can be parameterized in any way as long as $\pi(a|s, \bm{\theta})$ is differentiable w.r.t its parameters. In practice to ensure exploration we require the policy never to become deterministic, (i.e that $\pi(a|s, \bm{\theta})\in(0,1)$).

If the action space is Discrete (and not too large) then a natural and common kind of parameterization is to form parameterized numerical preferences $h(s,a,\bm{\theta})\in\mathbb{R}$ for each state-action pair.
Probability is assigned to actions proportionally to preferences, e.g., according to exponential soft-max distribution(called soft-max in action preferences)
    \begin{align*}
         \pi(a|s, \bm{\theta})\doteq\frac{e^{h(s,a,\bm{\theta})}}{\sum_b e^{h(s,b,\bm{\theta})}}
    \end{align*}

Preferences can themselves be parameterized arbitrarily for example they might be computed by a deep ANN. In this case $\theta$ is a vector of connection weights or the preferences could be linear in the features: $h(s,a,\bm{\theta})=\bm{\theta}^T\bm{x}(s,a)$ with $\bm{x}(s,a)\in \mathbb{R}^{d'}$ features of the policy.

Action values could be used as preferences with soft-max but this would not allow the policy to approach deterministic behaviours. Instead, general action preferences do not have to approach specific values allowing them to approach also deterministic policies.

\subsection{Advantages of Policy Parametrization over Action Value Parametrization}
When comparing policy parametrization with action value parametrization, the former offers several advantages:

\begin{enumerate}
\item The policy function is often simpler and easier to approximate compared to the action-value function.
\item Policy parametrization allows for the incorporation of prior knowledge, which is a significant benefit and a common reason for choosing policy-based learning methods.
\item Unlike action-value methods, policy gradient methods (such as those employing soft-max in action preferences) facilitate the selection of actions with arbitrary probabilities, thus supporting the use of stochastic policies.
\item Policy-based methods can effectively handle continuous action spaces, whereas action-value methods may face challenges in such scenarios due to their inability to directly handle continuous actions.
\end{enumerate}

\subsection{The Policy Gradient Theorem}
In addition to the practical advantages of policy parameterization over $\varepsilon$-greedy action selection, there is also an important theoretical advantage. With continuous policy
parameterization the action probabilities change smoothly as a function of the learned
parameter, whereas in $\varepsilon$-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values.
Largely due to this characteristic, policy-gradient methods offer stronger convergence guarantees compared to action-value methods.\\

With function approximation, it may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that \textbf{performance} depends on both the \textbf{action selections} and the \textbf{distribution of states} in which those selections are made, and that both of these are affected by the \textbf{policy parameter}. Given a state, the effect of the policy parameter on the actions, and thus on reward, can be computed in a relatively straightforward way from knowledge of the parameterization. But the effect of the policy on the state distribution is a function of the environment and is typically unknown. How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution?\\
\begin{center}
\begin{tikzpicture}[
roundnode/.style={circle, draw=black!60, fill=black!5, very thick, minimum size=7mm},
squarednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
]
    \node[squarednode]      (maintopic)                                                       {$J(\theta)$};
    \node[roundnode]        (rightcircle1)       [right=of maintopic]                         {$\pi(a)$};
    \node[roundnode]        (rightcircle2)       [right=of maintopic, below=of rightcircle1]  {$p(s)$};
    \node[roundnode]        (transition)         [right=of rightcircle2]                      {$T$};
    \node[squarednode]      (weights)            [right=of rightcircle1]                      {$\theta_{t+1}=\theta+\color{blue}?$};
    \node[]                 (text)               [right=of transition]                        {unknown};
    
    \draw[->] (maintopic.east)-- (rightcircle1.west);
    \draw[->] (maintopic.east) -- (rightcircle2.west);
    \draw[->] (rightcircle2.east) -- (transition.west);
    \draw[->] (rightcircle1.east) -- (weights.west);
    \draw[->] (rightcircle2.east) -- (weights.west);
    \draw[->, color=blue] (weights.north) to [out=150, in=40](maintopic.north);
    \draw[->] (text.west) -- (transition.east);
\end{tikzpicture}
\end{center}

\begin{tcolorbox}[colbacktitle=black!7!white,coltitle=black!75!white,title=\textbf{Proof of the Policy Gradient Theorem (episodic case)}]
With just elementary calculus and re-arranging of terms, we can prove the policy gradient theorem from first principles. To keep the notation simple, we leave it implicit in all cases that $\pi$ is a function of $\theta$, and all gradients are also implicitly with respect to $\theta$. First note that the gradient of the state-value function can be written in terms of the action-value function as
\begin{align*}
    \textcolor{blue}{\nabla v_\pi(s)}&=\nabla\Big[\sum_a\pi(a|s)q_\pi(s,a)\Big],\quad for\;all\;s\in S\\
    &=\sum_a\Big[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\nabla q_\pi(s,a)\Big]\\
    &=\sum_a\Big[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\nabla\sum_{s',r}p(s',r|s,a)(r+v_\pi(s') \Big]\\
    &=\sum_a\Bigg[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s'}p(s'|s,a)\textcolor{blue}{\nabla v_\pi(s')} \Bigg]\\
    &=\sum_a\Bigg[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s'}p(s'|s,a) \;\backslash \\
    &\quad\quad\textcolor{blue}{\sum_{a'}\Big[\nabla\pi(a'|s')q_\pi(s',a')+\pi(a'|s')\sum_{s''}p(s''|s',a')\nabla v_\pi(s'')\Big]}\Bigg]\\
    &=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty} Pr(s\rightarrow x, k, \pi)\sum_a\nabla\pi(a|x)q_\pi(x,a)
\end{align*}
after repeated unrolling, where $Pr(s\rightarrow x, k, \pi)$ is the probability of transitioning
from state s to state $x$ in $k$ steps under policy $\pi$. It is then immediate that
\begin{align*}
    \nabla J(\theta)&=\nabla v_\pi(s_0)\\
    &=\sum_{s}\Big(\sum_{k=0}^{\infty} Pr(s_0\rightarrow s, k, \pi)\Big)\sum_a\nabla\pi(a|s)q_\pi(s,a)\\
    &=\sum_{s}\eta(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)\\
    &=\textcolor{red}{\sum_{s'}\eta(s')}\sum_{s}\frac{\eta(s)}{\textcolor{red}{\sum_{s'}\eta(s')}}\sum_a\nabla\pi(a|s)q_\pi(s,a)\\
    &=\sum_{s'}\eta(s')\sum_{s}\mu(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)\quad\text{on-policy distribution}\; \mu(s)=\frac{\eta(s)}{\sum_{s'}\eta(s')}\\
    &\propto\sum_{s}\mu(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)
\end{align*}

Recall that
\begin{align*}
    v_\pi(s)&=\sum_a \pi(a|s)q_\pi(s,a)\\
    q_\pi(s, a)&=\sum_{s', r} p(s', r|s,a)(r+v_\pi(s'))
\end{align*}
\end{tcolorbox}
Fortunately, there is an excellent theoretical answer to this challenge in the form of the policy gradient theorem, which provides an analytic expression for the gradient of performance with respect to the policy parameter that does \textit{not} involve the derivative of the state distribution.

The policy gradient theorem for the episodic case establishes that:
\begin{align}
    \nabla J(\bm{\theta})\propto\sum_{s}\mu(s)\sum_a q_\pi(s,a)\nabla\pi(a|s, \bm{\theta})
\end{align}

\subsection{REINFORCE: Monte Carlo Policy Gradient}
Given the strategy of stochastic gradient ascent seen before
\begin{align}\label{eq:3}
\bm{\theta}_{t+1} = \bm{\theta}_t + \alpha \widehat{\nabla J(\bm{\theta}_t})
\end{align}
we need a way to obtain samples such that the expectation of the sample gradient $\widehat{\nabla J(\bm{\theta}_t})$ is proportional to the actual gradient $\nabla J(\bm{\theta}_t)$ the policy gradient theorem provides an exact expression proportional to the gradient, hence we use it for sampling from that expression.
We have that 
\begin{align}\label{eq:2}
    \nabla J(\bm{\theta})&\propto\sum_{s}\mu(s)\sum_a q_\pi(s,a)\nabla\pi(a|s, \bm{\theta})\\
    &=\mathbb{E}_\pi\Big[\sum_a q_\pi(S_t,a)\nabla\pi(a|S_t, \bm{\theta})\Big]
\end{align}
Notice that the right-hand side of the policy gradient theorem is a sum over states weighted by how often the states occur under the target policy $\pi$; if $\pi$ is followed, then states will be encountered in these proportions.
Then, we can instantiate a first stochastic gradient-ascent algorithm as
\begin{align}
    \bm{\theta}_{t+1}\doteq\bm{\theta}_t+\alpha\underbrace{\sum_a \hat{q}(S_t,a, \bm{w})\nabla\pi(a|S_t, \bm{\theta})}_{\widehat{\nabla J(\bm{\theta}_t})}
\end{align}
where is $\hat{q}$ some learned approximation of $q_\pi$.
We call this algorithm \textit{all-actions} because its update involves all of the actions.
If we consider instead only the action $A_t$ taken at time $t$ we obtain the \textbf{REINFORCE algorithm}.
We continue our derivation of REINFORCE by introducing $A_t$ in the same way as we introduced $S_t$ in \ref{eq:2} by replacing a sum over the random variable’s possible values by an expectation under $\pi$, and then sampling the expectation.
\begin{align*}
    \nabla J(\bm{\theta})&=\mathbb{E}_\pi\Big[\sum_a \textcolor{red}{\pi(a|S_t, \bm{\theta})}q_\pi(S_t,a)\frac{\nabla\pi(a|S_t, \bm{\theta})}{\textcolor{red}{\pi(a|S_t, \bm{\theta})}}\Big]\\
    &=\mathbb{E}_\pi\Big[q_\pi(S_t, A_t)\frac{\nabla\pi(A_t|S_t, \bm{\theta})}{\pi(A_t|S_t, \bm{\theta})}\Big] \qquad\text{(replacing $a$ by the sample $A_t\sim\pi$)}\\
    &=\mathbb{E}_\pi\Big[G_t\frac{\nabla\pi(A_t|S_t, \bm{\theta})}{\pi(A_t|S_t, \bm{\theta})}\Big] \qquad\text{because $\mathbb{E}_\pi[G_t|S_t,A_t]=q_\pi(S_t, A_t)$}
\end{align*}
where $G_t$ is the return as usual. The final expression in brackets is exactly what is needed, a quantity that can be sampled on each time step whose expectation is equal to the gradient. Using this sample to instantiate our generic stochastic gradient ascent algorithm \ref{eq:3} yields the REINFORCE update:
\begin{align}\label{eq:4}
\bm{\theta}_{t+1} = \bm{\theta}_t + \alpha G_t\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta})_t}
\end{align}
This update has an intuitive appeal. Each increment is proportional to the product of a return $G_t$ and the vector $\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta})_t}$. The vector is the direction in parameter space that most increases the probability of repeating the action $A_t$ on future visits to state $S_t$.
The update increases the parameter vector in this direction proportional to the return, and inversely proportional to the action probability. The former makes sense because it causes the parameter to move most in the directions that favor actions that yield the highest return. The latter makes sense because otherwise actions that are selected frequently are at an advantage (the updates will be more often in their direction) and might win out even if they do not yield the highest return.

Note that REINFORCE uses the complete return from time $t$, which includes all future rewards up until the end of the episode. In this sense REINFORCE is a Monte Carlo algorithm and is well defined only for the episodic case with all updates made in retrospect after the episode is completed.

\begin{tcolorbox}[colback=black!7!white,colframe=black!75!white,title=\textbf{REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_*$}]
Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$\\
Algorithm parameter: step size $\alpha > 0$\\
Initialize policy parameter $\bm{\theta}\in\mathbb{R}^d$ arbitrarily (e.g., to $\bm{0}$)\\


Loop forever (for each episode):

    \qquad Generate an episode $S_0, A_0, R_1, S_1, A_1,\dots,R_T,S_T$ using $\pi$

    \qquad Loop for each step of episode, $t=0,1,\dots, T-1$:

    \qquad\qquad $G\leftarrow\sum_{k=t+1}^T\gamma^{k-t-1}R_k$

    \qquad\qquad $\bm{\theta}\leftarrow \bm{\theta}+\alpha\gamma^tG\nabla\ln\pi(A_t|S_t, \bm{\theta})$
\end{tcolorbox}
Notice that the update in the last line of seudocode appears rather different from the REINFORCE update rule \ref{eq:4}. One difference is that the pseudocode uses the compact expression $\nabla\ln\pi(A_t|S_t, \bm{\theta})$ for the fractional vector $\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta})_t}$. 

That these two expressions for the vector are equivalent follows from the identity $\nabla\ln x=\frac{\nabla x}{x}$.
The second difference between the pseudocode update and the REINFORCE update equation is that the former includes a factor of $\gamma_t$. This is because for the derivation we are treating the non-discounted case ($\gamma=1$) while in the boxed
algorithms we are giving the algorithms for the general discounted case.\\

As a stochastic gradient method, REINFORCE has good theoretical convergence properties. By construction, the expected update over an episode is in the same direction as the performance gradient. This assures an improvement in expected performance for sufficiently small $\alpha$, and convergence to a local optimum under standard stochastic approximation conditions for decreasing $\alpha$. However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.

\subsection{REINFORCE with Baseline}
To solve the previously described problem of high variance the policy gradient theorem can be generalized to include a comparison of the
action value to an arbitrary baseline $b(s)$:
\begin{align}\label{eq:5}
    \nabla J(\bm{\theta})\propto\sum_{s}\mu(s)\sum_a \Big(q_\pi(s,a)-b(s)\Big)\nabla\pi(a|s, \bm{\theta})
\end{align}
The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains valid because the subtracted quantity is zero:
\begin{align*}
    \sum_a b(s)\nabla\pi(a|s, \bm{\theta})=b(s)\nabla\sum_a\pi(a|s, \bm{\theta})=b(s)\nabla 1=0
\end{align*}
The policy gradient theorem with baseline \ref{eq:5} can be used to derive an update
rule using similar steps as in the previous section. The update rule that we end up with
is a new version of REINFORCE that includes a general baseline:
\begin{align}\label{eq:6}
    \bm{\theta}_{t+1}\doteq\bm{\theta}_t+\alpha\Big(G_t-b(S_t)\Big)\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta}_t)}
\end{align}
Because the baseline could be uniformly zero, this update is a strict generalization of REINFORCE. In general, the baseline leaves the expected value of the update unchanged, but it can have a large effect on its variance.
One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t,\bm{w})$, where $\bm{w}\in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous chapters.
Because REINFORCE is a Monte Carlo method for learning the policy parameter, $\bm{\theta}$,
it seems natural to also use a Monte Carlo method to learn the state-value weights, $\bm{w}$.

\begin{tcolorbox}[colback=black!7!white,colframe=black!75!white,title=\textbf{REINFORCE with Baseline (episodic), for $\pi_\theta\approx\pi_* $}]
    Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$\\
    Input: a differentiable state-value function parameterization $\hat{v}(s,\bm{w})$\\
    Algorithm parameter: step sizes $\alpha^{\bm{\theta}} > 0, \alpha^{\bm{w}} > 0$\\
    Initialize policy parameter $\bm{\theta}\in\mathbb{R}^d$ and state-value weights $\bm{w}\in\mathbb{R}^d$  (e.g., to $\bm{0}$)\\
    
    Loop forever (for each episode):

        \qquad Generate an episode $S_0, A_0, R_1, S_1, A_1,\dots,R_T,S_T$ following $\pi(\cdot|\cdot, \bm{\theta})$

        \qquad Loop for each step of episode, $t=0,1,\dots, T-1$:

        \qquad\qquad $G\leftarrow\sum_{k=t+1}^T\gamma^{k-t-1}R_k$

        \qquad\qquad $\delta\leftarrow G-\hat{v}(S_t,\bm{w})$

        \qquad\qquad $\bm{w}\leftarrow\bm{w}+\alpha^{\bm{w}}\delta\nabla\hat{v}(S_t, \bm{w})$

        \qquad\qquad $\bm{\theta}\leftarrow \bm{\theta}+\alpha^{\bm{\theta}}\gamma^t\delta\nabla\ln\pi(A_t|S_t, \bm{\theta})$
\end{tcolorbox}

\subsection{Actor–Critic Methods}
Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor–critic method because its state-value function
is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), 
but only as a baseline for the state whose estimate is being updated.
This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence
on the quality of the function approximation. As we have seen, the bias introduced
through bootstrapping and reliance on the state representation is often beneficial because
it reduces variance and accelerates learning. REINFORCE with baseline is unbiased
and will converge asymptotically to a local minimum, but like all Monte Carlo methods
it tends to learn slowly (produce estimates of high variance) and to be inconvenient
to implement online or for continuing problems.\\

In order to overcome these drawbacks, Temporal-Difference (TD) techniques offer a solution by incorporating the benefits of TD into policy gradient methods. To achieve this, we introduce Actor-Critic methods with a Bootstrapping critic, which can be seen as a TD-based variation of Policy gradient.
One-step actor-critic methods can be viewed as the policy-gradient counterparts to the previously discussed TD methods such as TD, Sarsa, and Q-learning. What makes one-step methods particularly attractive is their complete online and incremental nature.
One-step actor–critic methods replace the full return
of REINFORCE (\ref{eq:6}) with the one-step return (and use a learned state-value function
as the baseline) as follows:
\begin{align*}
    \bm{\theta}_{t+1}&\doteq\bm{\theta}_{t}+\alpha\Big(\textcolor{red}{G_{t:t+1}}-\hat{v}(S_t, \bm{w})\Big)\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta}_t)}\\
    &=\bm{\theta}_{t}+\alpha\Big(\textcolor{red}{R_{t+1}+\gamma\hat{v}(S_{t+1}, \bm{w})}-\hat{v}(S_t, \bm{w})\Big)\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta}_t)}\\
    &=\bm{\theta}_{t}+\alpha\delta_t\frac{\nabla\pi(A_t|S_t, \bm{\theta}_t)}{\pi(A_t|S_t, \bm{\theta}_t)}
\end{align*}
The natural state-value-function learning method to pair with this is semi-gradient TD(0).
\newcommand\drawNodes[2]{
  % #1 (str): namespace
  % #2 (list[list[str]]): list of labels to print in the node of each neuron
  \foreach \neurons [count=\lyrIdx] in #2 {
    \StrCount{\neurons}{,}[\lyrLength] % use xstring package to save each layer size into \lyrLength macro
    \foreach \n [count=\nIdx] in \neurons
      \node[neuron] (#1-\lyrIdx-\nIdx) at (2*\lyrIdx, \lyrLength/2-1.4*\nIdx) {\n};
  }
}

\newcommand\denselyConnectNodes[2]{
  % #1 (str): namespace
  % #2 (list[int]): number of nodes in each layer
  \foreach \n [count=\lyrIdx, remember=\lyrIdx as \previdx, remember=\n as \prevn] in #2 {
    \foreach \y in {1,...,\n} {
      \ifnum \lyrIdx > 1
        \foreach \x in {1,...,\prevn}
          \draw[->] (#1-\previdx-\x) -- (#1-\lyrIdx-\y);
      \fi
    }
  }
}
\vspace{1cm}
\begin{tikzpicture}[
    shorten >=1pt, shorten <=1pt,
    neuron/.style={circle, draw, minimum size=4ex, thick},
    legend/.style={font=\large\bfseries},
  ]

  % actor
  \drawNodes{actor}{{{,,,,}, {,,,}, {,}}}
  \denselyConnectNodes{actor}{{5, 4, 2}}

  % input + output labels
  \foreach \idx in {1,...,5} {
      \node[left=0 of actor-1-\idx] {$x_\idx(s)$};
    }
  \foreach \idx in {1,...,2} {
      \node[right=0 of actor-3-\idx] {$p(a_\idx|s)$};
    }

  \node[above=0.1 of actor-1-1] {Actor network};
  \node[left=1.5 of actor-1-2, rotate=90] {\textbf{State}};
  \node[right=1.5 of actor-3-2, rotate=90] {\textbf{Actions}};

  % critic
  \begin{scope}[xshift=9cm]
    \drawNodes{critic}{{{,,,,}, {,,,}, {\\}}}
  \denselyConnectNodes{critic}{{5, 4, 1}}
  \end{scope}
  % input + output labels
  \foreach \idx in {1,...,5} {
      \node[left=0 of critic-1-\idx] {$x_\idx(s)$};
    }
  \node[right=0 of critic-3-1] {$\hat{v}(s)$};
  \node[above=0.1 of critic-1-1] {Critic network};
  \node[left=1.5 of critic-1-2, rotate=90] {\textbf{State}};
  \node[right=5 of critic-1-3, rotate=90] {\textbf{Value}};

\end{tikzpicture}

The actor net learns the policy function $\pi(A_t|S_t, \bm{\theta}_t)$ while the critic learn the value function $\hat{v}(S_t, \bm{w})$. 

In this setting the target becomes $R_{t+1}+\gamma\hat{v}(S_{t+1}, \bm{w})$ and the difference between the taget and the estimated value function ($R_{t+1}+\gamma\hat{v}(S_{t+1}, \bm{w})-\hat{v}(S_t, \bm{w})$) is called Advantage.
The advantage tells us if a state is better or worse than expected. If the action is better than expected - advantage $> 0$ - then we want to
incourage this action. If it is worse than expected - advantage $< 0$ - we want to encourage the opposite action.

\begin{tcolorbox}[colback=black!7!white,colframe=black!75!white,title=\textbf{One-step Actor-Critic (episodic), for $\pi_\theta\approx\pi_* $}]
    Input: a differentiable policy parameterization $\pi(a|s,\bm{\theta})$\\
    Input: a differentiable state-value function parameterization $\hat{v}(s,\bm{w})$\\
    Algorithm parameter: step sizes $\alpha^{\bm{\theta}} > 0, \alpha^{\bm{w}} > 0$\\
    Initialize policy parameter $\bm{\theta}\in\mathbb{R}^d$ and state-value weights $\bm{w}\in\mathbb{R}^d$  (e.g., to $\bm{0}$)\\
    
    Loop forever (for each episode):

        \qquad Initialize $S$ (first state of episode)

        \qquad $I\leftarrow 1$

        \qquad Loop while $S$ is not terminal (for each time step):

        \qquad\qquad Choose $A\sim\pi(\cdot|S, \bm{\theta})$
        
        \qquad\qquad Take action A, observe R, S'

        \qquad\qquad $\delta\leftarrow R+\gamma\hat{v}(S', \bm{w})-\hat{v}(S, \bm{w})$

        \qquad\qquad $\bm{w}\leftarrow\bm{w}+\alpha^{\bm{w}}\delta\nabla\hat{v}(S, \bm{w})$\qquad\qquad (if S' is terminal, then $\hat{v}(S', \bm{w})\doteq 0)$

        \qquad\qquad $\bm{\theta}\leftarrow \bm{\theta}+\alpha^{\bm{\theta}}\gamma^t\delta\nabla\ln\pi(A_t|S_t, \bm{\theta})$

        \qquad\qquad $I\leftarrow\gamma I$

        \qquad\qquad $S\leftarrow S'$
\end{tcolorbox}

\subsection{Policy Gradient for Continuing Problems}